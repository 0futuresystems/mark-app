Context
Repo: 0futuresystems/mark-app
Page: /new uses the in-app camera. On iPhones the photos look soft/blurry and low-detail. We need crisper photos without sacrificing speed.

Goals (must have)

Only one compression/downscale step in the whole pipeline.

Final saved image on iPhone: long edge ≥ 2048 px (prefer 2560 px when device can handle it).

JPEG quality ≥ 0.92 (default 0.95).

Keep the UI fast and responsive (no spinner longer than ~1s per shot on modern iPhones).

Prefer back camera; work over HTTPS; don’t break offline/PWA flow or IndexedDB storage.

What to change

Audit & remove double compression

Locate all image resize/compress calls (likely in src/components/MultiShotCamera.tsx and any helpers like src/lib/files.ts or similar).

Ensure we compress/resize only once. If we already resized at capture, skip any later downscale. Centralize into a single helper like processImage(file, { maxW: 2560, quality: 0.95 }) and call it only once.

Improve capture on iPhone

In MultiShotCamera.tsx (or the camera component), try progressive getUserMedia constraints:

First try width: { exact: 1920 }, height: { exact: 1080 }, frameRate: { ideal: 30 }.

If that fails, fall back to 1280x720, then to default facingMode: 'environment'.

iOS HQ fallback: add a path that uses the native capture input for high-res stills:

<input type="file" accept="image/*" capture="environment" />


Use getUserMedia for fast preview, but when the user taps “Take picture” on iOS, prefer the native file input to grab a full-res still (typically 3024×4032). If native input is canceled/blocked, fall back to the video frame capture.

Canvas/bitmap quality

When capturing from video, set canvas size from video.videoWidth/video.videoHeight and cap long edge at 2560 (configurable). Keep aspect ratio.

Use devicePixelRatio correctly: set the canvas pixel width/height to the target resolution (don’t rely on CSS scaling).

Use ctx.imageSmoothingQuality = 'high' (keep smoothing enabled).

Export JPEG once at quality 0.95 (configurable).

Skip re-processing already-processed images

In the central processor (e.g., processImage), if the image is already ≤ maxW and type is JPEG, return as-is.

Remove/disable any earlier downscaleImage(..., maxW: 1600, quality: 0.82) calls so we don’t double-compress.

UX & speed

Keep instant preview. On iOS, after shutter, show a tiny “Capturing high-quality photo…” toast while the native input returns, then place the HQ image.

If we fall back to video frame capture, show a small “Optimized quality (fallback)” note so the user knows it’s still good.

Edge cases

Enforce HTTPS for camera (keep existing checks) but don’t show confusing dev prompts.

Make sure orientation is correct (rotate based on EXIF for native file input; video frame usually doesn’t need EXIF handling).

Don’t break offline storage and current upload flow (IndexedDB → R2).

Acceptance criteria

Taking a photo on iPhone 12/13/14/15 over HTTPS yields:

Dimensions: long edge ≥ 2048 px (prefer 2560 px when using native input).

Single JPEG encode, quality ≥ 0.92 (default 0.95).

Resulting file size typically 1–3 MB (varies with scene).

No noticeable blur vs current build; text on boxes/labels should be clearly readable when zoomed.

No regressions to offline/PWA behavior.

Testing checklist

iPhone Safari (physical device):

/new loads preview quickly.

Tap “Take a picture” → see HQ image; confirm saved file dimensions & size.

Deny native input → fallback still works and is sharper than before.

Confirm only one compression pass (add a temporary console log at the central processor and remove after verifying).

Confirm images still save and show in the review grid and can be uploaded as before.

Deliverables

Commit with:

Centralized image processor (name it clearly).

Updated camera component with progressive constraints + iOS native capture fallback.

Removal of redundant compression/downscale calls.